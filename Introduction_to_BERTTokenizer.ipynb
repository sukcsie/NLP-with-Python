{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction_to_BERTTokenizer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN0hCsWtreHGzLHdmw2jNwG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sukcsie/NLP-with-Python/blob/main/Introduction_to_BERTTokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCocvwtqbaVF"
      },
      "source": [
        "This is a quick tutorial to get started with BERT tokenizer and get your text data ready for your Deep learning models. I have used `transformers` library which is the State-of-the-art Natural Language Processing for *Pytorch* and *TensorFlow 2.0*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sy9q6ooQ2el7"
      },
      "source": [
        "# if the package is not installed, then install it\n",
        "try:\n",
        "    import transformers\n",
        "except:\n",
        "    !pip install transformers\n",
        "    import transformers\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NepWbp_W2W3O",
        "outputId": "731935f2-4fa4-4a0a-d1ba-14174587a298"
      },
      "source": [
        "# BERT tokenizer\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer.tokenize(\"I am trying to get sentences ready for BERT!\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'am', 'trying', 'to', 'get', 'sentences', 'ready', 'for', 'bert', '!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEcKvO7NW-xj",
        "outputId": "5b32e53e-b7d4-418c-f800-21007951bd3a"
      },
      "source": [
        "max_length_test = 50\n",
        "test_sentence = 'I am trying out different options to learn BERT tokenizers. This is an example. Feel free to use this code as you wish!'\n",
        "tokenized = tokenizer.tokenize(test_sentence)\n",
        "print('tokenized', tokenized)\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenized ['i', 'am', 'trying', 'out', 'different', 'options', 'to', 'learn', 'bert', 'token', '##izer', '##s', '.', 'this', 'is', 'an', 'example', '.', 'feel', 'free', 'to', 'use', 'this', 'code', 'as', 'you', 'wish', '!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJvPiRejXy2P",
        "outputId": "30fdbabc-72bd-4c82-cc0b-4ff19bc62183"
      },
      "source": [
        "# adding special tokens\n",
        "# [CLS] is used at the beginning of a sentence\n",
        "# [SEP] is used at the end of a sentence\n",
        "test_sentence_with_special_tokens = '[CLS]' + test_sentence + '[SEP]'\n",
        "tokenized = tokenizer.tokenize(test_sentence_with_special_tokens)\n",
        "print('tokenized', tokenized)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenized ['[CLS]', 'i', 'am', 'trying', 'out', 'different', 'options', 'to', 'learn', 'bert', 'token', '##izer', '##s', '.', 'this', 'is', 'an', 'example', '.', 'feel', 'free', 'to', 'use', 'this', 'code', 'as', 'you', 'wish', '!', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Zi9DFURYA3E",
        "outputId": "93fc5ef1-a1e6-4135-cbcf-34241e38a1a9"
      },
      "source": [
        "# convert tokens to ids in WordPiece\n",
        "input_ids = tokenizer.convert_tokens_to_ids(tokenized)\n",
        "print('input IDs', input_ids)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input IDs [101, 1045, 2572, 2667, 2041, 2367, 7047, 2000, 4553, 14324, 19204, 17629, 2015, 1012, 2023, 2003, 2019, 2742, 1012, 2514, 2489, 2000, 2224, 2023, 3642, 2004, 2017, 4299, 999, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ee26qMmIYIFc",
        "outputId": "d24dc67e-9e70-475a-e0c5-a1710ef61fe7"
      },
      "source": [
        "# precalculation of pad length, so that we can reuse it later on\n",
        "padding_length = max_length_test - len(input_ids)\n",
        "print('padding length', padding_length)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "padding length 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJEyZr_AYKIi",
        "outputId": "8f68e55b-d86e-4c3b-eb84-a9de64b97fd8"
      },
      "source": [
        "# map tokens to WordPiece dictionary and add pad token for those text shorter than our max length\n",
        "input_ids = input_ids + ([0] * padding_length)\n",
        "print('padded input ids', input_ids)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "padded input ids [101, 1045, 2572, 2667, 2041, 2367, 7047, 2000, 4553, 14324, 19204, 17629, 2015, 1012, 2023, 2003, 2019, 2742, 1012, 2514, 2489, 2000, 2224, 2023, 3642, 2004, 2017, 4299, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SP6uE6VOYM5L",
        "outputId": "4a7729c9-a209-4290-b584-b152c428134e"
      },
      "source": [
        "# attention should focus just on sequence with non padded tokens\n",
        "attention_mask = [1] * len(input_ids)\n",
        "print('attention mask', attention_mask)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "attention mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60B-OjhbYSiD",
        "outputId": "11fa1a8b-1c48-4650-f1e4-b2e5c21a66d2"
      },
      "source": [
        "# do not focus attention on padded tokens\n",
        "attention_mask = attention_mask + ([0] * padding_length)\n",
        "print('attention mask not focusing on padded tokens', attention_mask)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "attention mask not focusing on padded tokens [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1pU4iv7ZYYP",
        "outputId": "971a20d9-ff1f-4a92-efb9-5439de573ba9"
      },
      "source": [
        "# token types, needed for example for question answering, for our purpose we will just set 0 as we have just one sequence\n",
        "token_type_ids = [0] * max_length_test\n",
        "bert_input = {\n",
        "    \"token_ids\": input_ids,\n",
        "    \"token_type_ids\": token_type_ids,\n",
        "    \"attention_mask\": attention_mask\n",
        "} \n",
        "print(bert_input)\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'token_ids': [101, 1045, 2572, 2667, 2041, 2367, 7047, 2000, 4553, 14324, 19204, 17629, 2015, 1012, 2023, 2003, 2019, 2742, 1012, 2514, 2489, 2000, 2224, 2023, 3642, 2004, 2017, 4299, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDI5SKirZqZm",
        "outputId": "54a171a1-8275-420f-8638-88d2068b3cc6"
      },
      "source": [
        "# Now in the practical coding we will use just encode_plus function, which does all of those steps for us\n",
        "\n",
        "bert_input = tokenizer.encode_plus(\n",
        "                        test_sentence,                      \n",
        "                        add_special_tokens = True, # add [CLS], [SEP]\n",
        "                        max_length = max_length_test, # max length of the text that can go to BERT\n",
        "                        pad_to_max_length = True, # add [PAD] tokens\n",
        "                        return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
        "              )\n",
        "print('Encoded', bert_input)\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoded {'input_ids': [101, 1045, 2572, 2667, 2041, 2367, 7047, 2000, 4553, 14324, 19204, 17629, 2015, 1012, 2023, 2003, 2019, 2742, 1012, 2514, 2489, 2000, 2224, 2023, 3642, 2004, 2017, 4299, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}