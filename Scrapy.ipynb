{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scrapy.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sukcsie/NLP-with-Python/blob/main/Scrapy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eggPUzlRiJtH"
      },
      "source": [
        "### Using Scrapy in Google colab notebook \n",
        "I have created this tutorial taking ideas from [Scrapy's](https://docs.scrapy.org/en/latest/intro/tutorial.html) tutorial page and [stackoverflow](https://stackoverflow.com/questions/40856730/how-to-run-scrapy-project-in-jupyter) post, especially how to run scrapy in your `notebook` instead of installing independently in your machines. \n",
        "\n",
        "I have implemented a basic user-written crawler to crawl the pages and have shown how to represent the data read in Pandas data frame. It used `Scrapy` library to crawl and scrape data from a website.\n",
        "\n",
        "Once the quotes are retrieved the JSON file will be created on disk (in the cloud for this case) and can be loaded to a Pandas dataframe. This dataframe can then be analyzed, modified and be used for further processing. This notebook simply loads the JSON file to a dataframe and writes it again to a pickle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klr0kFyWfW6c"
      },
      "source": [
        "# Settings for notebook\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "# Show Python version\n",
        "import platform\n",
        "platform.python_version()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH9T63poftsB"
      },
      "source": [
        "#### Import scrapy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6irGit7Afm3B"
      },
      "source": [
        "try:\n",
        "    import scrapy\n",
        "except:\n",
        "    !pip install scrapy\n",
        "    import scrapy\n",
        "from scrapy.crawler import CrawlerProcess"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVw6m4Nbf6b6"
      },
      "source": [
        "#### Set up a pipeline\n",
        "This class creates a simple pipeline that writes all found items to a JSON file, where each line contains one JSON element.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bn90WoMvfm-x"
      },
      "source": [
        "\n",
        "import json\n",
        "\n",
        "class JsonWriterPipeline(object):\n",
        "\n",
        "    def open_spider(self, spider):\n",
        "        self.file = open('quoteresult.jl', 'w')\n",
        "\n",
        "    def close_spider(self, spider):\n",
        "        self.file.close()\n",
        "\n",
        "    def process_item(self, item, spider):\n",
        "        line = json.dumps(dict(item)) + \"\\n\"\n",
        "        self.file.write(line)\n",
        "        return item"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTJW5ZVAgD_S"
      },
      "source": [
        "#### Define the spider\n",
        "Define the spider\n",
        "The `MySpider` class defines from which URLs to start crawling and which values to retrieve. I set the logging level of the crawler to warning, otherwise the notebook is overloaded with DEBUG messages about the retrieved data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jqnvk8SvgAkb"
      },
      "source": [
        "\n",
        "import logging\n",
        "\n",
        "class MySpider(scrapy.Spider):\n",
        "    name = \"quotes\"\n",
        "    start_urls = [\n",
        "        'http://quotes.toscrape.com/page/1/',\n",
        "        'http://quotes.toscrape.com/page/2/',\n",
        "    ]\n",
        "    custom_settings = {\n",
        "        'LOG_LEVEL': logging.WARNING,\n",
        "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}, # Used for pipeline 1\n",
        "        'FEED_FORMAT':'json',                                 # Used for pipeline 2\n",
        "        'FEED_URI': 'quoteresult.json'                        # Used for pipeline 2\n",
        "    }\n",
        "    \n",
        "    def parse(self, response):\n",
        "        for quote in response.css('div.quote'):\n",
        "            yield {\n",
        "                'text': quote.css('span.text::text').extract_first(),\n",
        "                'author': quote.css('span small::text').extract_first(),\n",
        "                'tags': quote.css('div.tags a.tag::text').extract(),\n",
        "            }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BoVGBBygZye"
      },
      "source": [
        "#### Start the crawler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GR8gUTSgenX"
      },
      "source": [
        "process = CrawlerProcess({\n",
        "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
        "})\n",
        "\n",
        "process.crawl(MySpider)\n",
        "process.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHW6-nAbgleL"
      },
      "source": [
        "#### Check the files\n",
        "Verify that the files has been created on disk. As we can observe the files are both created and have data. The .jl file has line separated JSON elements, while the .json file has one big JSON array containing all the quotes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feEw2kwAgvtu"
      },
      "source": [
        "ll quoteresult.*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dt1oz63KqLsW"
      },
      "source": [
        "#!rm quoteresult.*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HG_1pQ9MhJNj"
      },
      "source": [
        "# executing a shell command on notebook \n",
        "#!tail -n 2 quoteresult.jl\n",
        "!cat quoteresult.jl\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDkPp-QThMTe"
      },
      "source": [
        "# executing a shell command on notebook \n",
        "!tail -n 2 quoteresult.json\n",
        "#!cat quoteresult.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCfaFGOshZ02"
      },
      "source": [
        "#### Create dataframes\n",
        "Pandas can now be used to create dataframes and save the frames to pickles. The .json file can be loaded directly into a frame, whereas for the .jl file we need to specify the JSON objects are divided per line.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2hz60O8hUKU"
      },
      "source": [
        "import pandas as pd\n",
        "#dfjson = pd.read_json('quoteresult.json')\n",
        "#dfjson"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTz8srmghuAG"
      },
      "source": [
        "dfjl = pd.read_json('quoteresult.jl', lines=True)\n",
        "dfjl\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lvLYFXuQrbt"
      },
      "source": [
        "#### Accessing Google drive and keeping a copy of the file generated through scraping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HH17pFfPS69"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYKcbY1RPY3l"
      },
      "source": [
        "!ls /content/gdrive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAPfIEFaPb7Q"
      },
      "source": [
        "import shutil\n",
        "shutil.copyfile('quoteresult.jl', '/content/gdrive/My Drive/Colab Notebooks/quote2.jl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPQdJYz9PieN"
      },
      "source": [
        "!cat '/content/gdrive/My Drive/Colab Notebooks/quote2.jl'"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}