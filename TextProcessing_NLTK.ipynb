{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.13"
    },
    "colab": {
      "name": "TextProcessing_NLTK.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sukcsie/NLP-with-Python/blob/main/TextProcessing_NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VePkXOAu8C7"
      },
      "source": [
        "### Text Processing using NLTK\n",
        "The main aim of this in-class exercise is to make you conversant with different text pre-processing tasks like `tokenization`, `stemming`, `stopword removal` using NLTK."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDxhwOqvu8C8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d0e2682-e868-478f-f13c-6e237cc658b8"
      },
      "source": [
        "# Import library nltk\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.text import Text\n",
        "from nltk.stem import PorterStemmer \n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b86PZWaIu8DA"
      },
      "source": [
        "# Importing specifics\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "# example sentence\n",
        "example = \"This is a sample sentence, showing different NLP concepts. We are doing this for web and data mining to learn text processing.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ws7vUSqOu8DC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb55bce1-78d8-4e25-d860-51ff77fbc454"
      },
      "source": [
        "# tokenize the example and store into the variable sent_tokens\n",
        "sent_tokens = sent_tokenize(example)\n",
        "# print the tokens\n",
        "print(len(sent_tokens))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8ZnGmtru8DG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99ca2679-53d1-4435-8e39-1202c02e9331"
      },
      "source": [
        "# tokenize the example corpus into word_token\n",
        "word_token = word_tokenize(example)\n",
        "# print the word tokens\n",
        "print (word_token)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'different', 'NLP', 'concepts', '.', 'We', 'are', 'doing', 'this', 'for', 'web', 'and', 'data', 'mining', 'to', 'learn', 'text', 'processing', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaQ7Q9ouu8DI"
      },
      "source": [
        "# invoking stopwords present in the library\n",
        "#example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
        "#from nltk.corpus import stopwords\n",
        "#stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "#word_tokens = word_tokenize(example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a59z3kT4u8DL"
      },
      "source": [
        "# import stop words\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5rU7zCBu8DN"
      },
      "source": [
        "# create a list filtered_sentence\n",
        "# that will hold all the words in the corpus except for the stopwords\n",
        "#filtered_sentence = [w for w in word_token if not w in stop_words]\n",
        "\n",
        "filtered_sentence = []\n",
        "\n",
        "for w in word_token:\n",
        "    if w.lower() not in stop_words:\n",
        "        filtered_sentence.append(w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJdEPoItu8DQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65340a3f-e4e0-4393-be9e-6664d7b9779e"
      },
      "source": [
        "# print filtered sentence\n",
        "print filtered_sentence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['sample', 'sentence', ',', 'showing', 'different', 'NLP', 'concepts', '.', 'web', 'data', 'mining', 'learn', 'text', 'processing', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6TdpVl5u8DT"
      },
      "source": [
        "# import Porter Stemmer\n",
        "from nltk.stem import PorterStemmer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evuB8dYPu8DV"
      },
      "source": [
        "ps = PorterStemmer()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8_gTxXLu8DY"
      },
      "source": [
        "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pWhVslFu8Da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1aecbab-6e29-4da7-f91a-4fffecd14636"
      },
      "source": [
        "# stem the words given in the list above and print them\n",
        "for w in example_words:\n",
        "    print(ps.stem(w))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python\n",
            "python\n",
            "python\n",
            "python\n",
            "pythonli\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPV6x7qZu8Dc"
      },
      "source": [
        "new_text = \"It is important to by very pythonly while you are pythoning with python. All pythoners have pythoned poorly at least once.\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeXKnVNUu8Df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17fc6b35-603d-437a-9dfa-238c6d944d52"
      },
      "source": [
        "words = word_tokenize(new_text)\n",
        "\n",
        "for w in words:\n",
        "    print(ps.stem(w))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "It\n",
            "is\n",
            "import\n",
            "to\n",
            "by\n",
            "veri\n",
            "pythonli\n",
            "while\n",
            "you\n",
            "are\n",
            "python\n",
            "with\n",
            "python\n",
            ".\n",
            "all\n",
            "python\n",
            "have\n",
            "python\n",
            "poorli\n",
            "at\n",
            "least\n",
            "onc\n",
            ".\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEc1jzdeu8Di"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}